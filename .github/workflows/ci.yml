# =============================================================================
# TARJETA CRC - .github/workflows/ci.yml (Pipeline de CI)
# =============================================================================
# Responsabilidades:
# - Ejecutar lint/tests y checks de contratos para backend y frontend.
# - Verificar que no existan secretos versionados.
# - Ejecutar e2e y load tests segun reglas.
#
# Colaboradores:
# - scripts/security/verify_no_secrets.sh
# - shared/contracts (OpenAPI + orval)
# - apps/backend / apps/frontend
#
# Invariantes:
# - No imprimir secretos en logs (usar redaccion).
# - Mantener compatibilidad con branches main/develop.
# =============================================================================

name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  secret-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Verify no secrets
        run: bash scripts/security/verify_no_secrets.sh
      - name: Gitleaks scan (redacted)
        uses: gitleaks/gitleaks-action@v2
        with:
          args: --redact --no-banner --config=.gitleaks.toml
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  kustomize-render:
    runs-on: ubuntu-latest
    env:
      BACKEND_IMAGE: ghcr.io/${{ github.repository }}/backend:sha-${{ github.sha }}
      FRONTEND_IMAGE: ghcr.io/${{ github.repository }}/frontend:sha-${{ github.sha }}
    steps:
      - uses: actions/checkout@v4
      - name: Render staging overlay
        run: |
          bash infra/k8s/render_kustomize.sh staging \
            --backend-image "$BACKEND_IMAGE" \
            --frontend-image "$FRONTEND_IMAGE" \
            --out /tmp/ragcorp-staging.yaml
      - name: Render prod overlay
        run: |
          bash infra/k8s/render_kustomize.sh prod \
            --backend-image "$BACKEND_IMAGE" \
            --frontend-image "$FRONTEND_IMAGE" \
            --out /tmp/ragcorp-prod.yaml
      - name: Validate rendered outputs
        run: |
          test -s /tmp/ragcorp-staging.yaml
          test -s /tmp/ragcorp-prod.yaml
          if grep -n ":latest" /tmp/ragcorp-prod.yaml; then
            echo "::error::Prod overlay contiene :latest. Usar tags inmutables."
            exit 1
          fi
      - name: Upload rendered manifests
        uses: actions/upload-artifact@v4
        with:
          name: kustomize-render
          path: |
            /tmp/ragcorp-staging.yaml
            /tmp/ragcorp-prod.yaml

  backend-lint:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: apps/backend
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
      - run: pip install ruff
      - run: ruff check .
      - run: ruff format --check .

  backend-test:
    runs-on: ubuntu-latest
    needs: backend-lint
    defaults:
      run:
        working-directory: apps/backend
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: ragcorp_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql://test:test@localhost:5432/ragcorp_test
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
      POSTGRES_DB: ragcorp_test
      POSTGRES_HOST: localhost
      POSTGRES_HOST_PORT: "5432"
      PGUSER: test
      PGPASSWORD: test
      PGDATABASE: ragcorp_test
      PGHOST: localhost
      PGPORT: "5432"
      GOOGLE_API_KEY: fake-key-for-tests
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
      - run: pip install -r requirements.txt
      - name: Enable pgvector extension
        run: |
          python - <<'PY'
          import os
          import psycopg

          url = os.environ["DATABASE_URL"]
          with psycopg.connect(url, autocommit=True) as conn:
              conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
          PY
      - run: pytest --cov=app --cov-report=xml -q
      - uses: codecov/codecov-action@v4
        with:
          files: apps/backend/coverage.xml
        continue-on-error: true

  frontend-lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10.0.0
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
          cache-dependency-path: pnpm-lock.yaml
      - run: pnpm install --frozen-lockfile
      - run: pnpm --filter web lint
      - run: pnpm --filter web exec tsc --noEmit

  frontend-test:
    runs-on: ubuntu-latest
    needs: frontend-lint
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10.0.0
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
          cache-dependency-path: pnpm-lock.yaml
      - run: pnpm install --frozen-lockfile
      - run: pnpm --filter web test --coverage
      - uses: codecov/codecov-action@v4
        with:
          files: apps/frontend/coverage/lcov.info
        continue-on-error: true

  docker-build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build backend image (gate)
        uses: docker/build-push-action@v5
        with:
          context: ./apps/backend
          file: ./apps/backend/Dockerfile
          push: false
          cache-from: type=gha
          cache-to: type=gha,mode=max
      - name: Build frontend image (gate)
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./apps/frontend/Dockerfile
          push: false
          cache-from: type=gha
          cache-to: type=gha,mode=max

  contracts-check:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      DATABASE_URL: postgresql://test:test@localhost:5432/ragcorp_test
      GOOGLE_API_KEY: fake-key-for-tests
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
      - uses: pnpm/action-setup@v4
        with:
          version: 10.0.0
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
      - run: |
          cd apps/backend && pip install -r requirements.txt
          python scripts/export_openapi.py --out ../../shared/contracts/openapi.json
      - run: pnpm install --frozen-lockfile
      - run: pnpm contracts:gen
      - name: Commit and push changes
        run: |
          if ! git diff --quiet shared/contracts/; then
            git config --global user.name 'github-actions[bot]'
            git config --global user.email 'github-actions[bot]@users.noreply.github.com'
            git add shared/contracts/
            git commit -m "chore(contracts): sync generated contracts [skip ci]"
            git push
          fi

  e2e:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository)
    timeout-minutes: 35
    env:
      FAKE_LLM: "1"
      FAKE_EMBEDDINGS: "1"
      API_KEYS_CONFIG: '{"e2e-key":["ingest","ask"]}'
      TEST_API_KEY: e2e-key
      E2E_USE_COMPOSE: "1"
      E2E_BASE_URL: http://localhost:3000
      E2E_API_URL: http://localhost:8000
      E2E_ADMIN_EMAIL: admin@example.com
      NEXT_PUBLIC_API_URL: http://rag-api:8000
      S3_ENDPOINT_URL: http://minio:9000
      S3_BUCKET: rag-documents
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
      WORKER_HTTP_PORT: "8001"
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10.0.0
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
      - run: pnpm install --frozen-lockfile
      - name: Install Playwright dependencies
        run: |
          pnpm e2e:install
          pnpm -C tests/e2e exec playwright install --with-deps
      - name: Prepare E2E credentials
        run: |
          if [[ -z "${E2E_ADMIN_PASSWORD:-}" ]]; then
            echo "E2E_ADMIN_PASSWORD=$(openssl rand -hex 16)" >> "$GITHUB_ENV"
          fi
          if [[ -z "${MINIO_ROOT_PASSWORD:-}" ]]; then
            echo "MINIO_ROOT_PASSWORD=minioadmin" >> "$GITHUB_ENV"
          fi
          if [[ -z "${S3_ACCESS_KEY:-}" ]]; then
            echo "S3_ACCESS_KEY=minioadmin" >> "$GITHUB_ENV"
          fi
          if [[ -z "${S3_SECRET_KEY:-}" ]]; then
            echo "S3_SECRET_KEY=minioadmin" >> "$GITHUB_ENV"
          fi
      - name: Start stack
        run: docker compose --profile e2e --profile worker --profile storage up -d --build
      - name: Wait for backend
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:8000/healthz; then
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:8000/healthz
      - name: Wait for worker readiness
        run: |
          for i in {1..30}; do
            if docker compose exec -T worker python -c "import urllib.request as u; u.urlopen('http://localhost:8001/readyz')"; then
              break
            fi
            sleep 2
          done
          docker compose exec -T worker python -c "import urllib.request as u; print(u.urlopen('http://localhost:8001/readyz').read().decode())"
      - name: Wait for MinIO readiness
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:9000/minio/health/ready; then
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:9000/minio/health/ready
      - name: Bootstrap admin (E2E)
        run: docker compose exec -T rag-api python scripts/create_admin.py --email "$E2E_ADMIN_EMAIL" --password "$E2E_ADMIN_PASSWORD"
      - name: Wait for web
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:3000; then
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:3000
      - name: Final stabilization wait
        run: sleep 5
      - name: Run Playwright tests
        run: pnpm -C tests/e2e test
      - name: Upload Playwright artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-artifacts
          path: |
            tests/e2e/playwright-report
            tests/e2e/test-results
      - name: Tear down stack
        if: always()
        run: docker compose --profile e2e --profile worker --profile storage down -v

  e2e-full:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository)
    timeout-minutes: 45
    env:
      FAKE_LLM: "1"
      FAKE_EMBEDDINGS: "1"
      E2E_USE_COMPOSE: "1"
      E2E_BASE_URL: http://localhost:3000
      E2E_API_URL: http://localhost:8000
      NEXT_PUBLIC_API_URL: http://rag-api:8000
      E2E_ADMIN_EMAIL: admin@example.com
      S3_ENDPOINT_URL: http://minio:9000
      S3_BUCKET: rag-documents
      WORKER_HTTP_PORT: "8001"
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10.0.0
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: pnpm
      - run: pnpm install --frozen-lockfile
      - name: Install Playwright dependencies
        run: |
          pnpm e2e:install
          pnpm -C tests/e2e exec playwright install --with-deps
      - name: Prepare E2E credentials
        run: |
          if [[ -z "${E2E_ADMIN_PASSWORD:-}" ]]; then
            echo "E2E_ADMIN_PASSWORD=$(openssl rand -hex 16)" >> "$GITHUB_ENV"
          fi
          if [[ -z "${MINIO_ROOT_USER:-}" ]]; then
            echo "MINIO_ROOT_USER=minioadmin" >> "$GITHUB_ENV"
          fi
          if [[ -z "${MINIO_ROOT_PASSWORD:-}" ]]; then
            echo "MINIO_ROOT_PASSWORD=minioadmin" >> "$GITHUB_ENV"
          fi
          if [[ -z "${S3_ACCESS_KEY:-}" ]]; then
            echo "S3_ACCESS_KEY=minioadmin" >> "$GITHUB_ENV"
          fi
          if [[ -z "${S3_SECRET_KEY:-}" ]]; then
            echo "S3_SECRET_KEY=minioadmin" >> "$GITHUB_ENV"
          fi
      - name: Start full stack
        run: docker compose --profile e2e --profile worker --profile storage up -d --build
      - name: Wait for backend
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:8000/healthz; then
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:8000/healthz
      - name: Wait for worker readiness
        run: |
          for i in {1..30}; do
            if docker compose exec -T worker python -c "import urllib.request as u; u.urlopen('http://localhost:8001/readyz')"; then
              break
            fi
            sleep 2
          done
          docker compose exec -T worker python -c "import urllib.request as u; print(u.urlopen('http://localhost:8001/readyz').read().decode())"
      - name: Wait for MinIO readiness
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:9000/minio/health/ready; then
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:9000/minio/health/ready
      - name: Bootstrap admin
        run: docker compose exec -T rag-api python scripts/create_admin.py --email "$E2E_ADMIN_EMAIL" --password "$E2E_ADMIN_PASSWORD"
      - name: Run Playwright full pipeline tests
        run: pnpm -C tests/e2e test --grep "Full pipeline"
      - name: Upload Playwright artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-full-artifacts
          path: |
            tests/e2e/playwright-report
            tests/e2e/test-results
      - name: Tear down stack
        if: always()
        run: docker compose --profile e2e --profile worker --profile storage down -v

  load-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: ragcorp_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql://test:test@localhost:5432/ragcorp_test
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
      POSTGRES_DB: ragcorp_test
      POSTGRES_HOST: localhost
      POSTGRES_HOST_PORT: "5432"
      PGUSER: test
      PGPASSWORD: test
      PGDATABASE: ragcorp_test
      PGHOST: localhost
      PGPORT: "5432"
      FAKE_LLM: "1"
      FAKE_EMBEDDINGS: "1"
      GOOGLE_API_KEY: fake-key-for-ci
      E2E_SEED_ADMIN: "1"
      E2E_ADMIN_EMAIL: admin@example.com
      E2E_ADMIN_PASSWORD: admin
      K6_ADMIN_EMAIL: admin@example.com
      K6_ADMIN_PASSWORD: admin
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
      - name: Install backend dependencies
        run: cd apps/backend && pip install -r requirements.txt
      - name: Enable pgvector extension
        run: |
          cd apps/backend
          python - <<'PY'
          import os
          import psycopg

          url = os.environ["DATABASE_URL"]
          with psycopg.connect(url, autocommit=True) as conn:
              conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
          PY
      - name: Run database migrations
        run: cd apps/backend && alembic upgrade head
      - name: Start API server
        run: |
          cd apps/backend
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          echo "Waiting for API healthz..."
          for i in {1..30}; do
            if curl -sf http://localhost:8000/healthz; then
              echo "API ready"
              break
            fi
            sleep 2
          done
          curl -sf http://localhost:8000/healthz
      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      - name: Run load tests
        run: k6 run tests/load/api.k6.js --env BASE_URL=http://localhost:8000 --summary-export=k6-summary.json
      - name: Upload k6 results
        uses: actions/upload-artifact@v4
        with:
          name: k6-results
          path: k6-summary.json
        if: always()
