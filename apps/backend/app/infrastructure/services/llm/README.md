# Infra: LLM Services (Generative AI)

## ğŸ¯ MisiÃ³n

Implementaciones concretas de los Modelos de Lenguaje (GeneraciÃ³n de texto).
Transforma prompts (strings) en respuestas (strings o streams).

**QuÃ© SÃ hace:**

- Cliente para Google Gemini (`google_llm_service.py`).
- Cliente Mock (`fake_llm.py`) para tests sin costo.

**QuÃ© NO hace:**

- No construye el prompt (eso es `application/context_builder.py`).

## ğŸ—ºï¸ Mapa del territorio

| Recurso                 | Tipo       | Responsabilidad (en humano)                                 |
| :---------------------- | :--------- | :---------------------------------------------------------- |
| `fake_llm.py`           | ğŸ Archivo | Simula un LLM repitiendo el input o devolviendo texto fijo. |
| `google_llm_service.py` | ğŸ Archivo | Adaptador para Google Generative AI (Gemini Pro).           |

## âš™ï¸ Â¿CÃ³mo funciona por dentro?

Implementa `LLMService` del Dominio.
Debe soportar dos modos:

1.  `generate(prompt) -> str`: Bloqueante.
2.  `generate_stream(prompt) -> Iterator[str]`: Streaming de tokens.

## ğŸ”— Conexiones y roles

- **Rol ArquitectÃ³nico:** Infrastructure Adapter.
- **Llama a:** SDKs de proveedores (google-generativeai).

## ğŸ‘©â€ğŸ’» GuÃ­a de uso (Snippets)

### GeneraciÃ³n simple

```python
llm = GoogleLLMService(api_key="...", model="gemini-pro")
respuesta = llm.generate("Â¿Capital de Francia?")
```

## ğŸ§© CÃ³mo extender sin romper nada

1.  **Nuevo Modelo:** Si agregas OpenAI GPT-4, asegÃºrate de implementar tanto `generate` como `generate_stream`.

## ğŸ†˜ Troubleshooting

- **SÃ­ntoma:** Error 429 (Resource Exhausted).
  - **Causa:** Cuota de API excedida. El sistema de `retry.py` deberÃ­a manejarlo, pero si persiste, aumenta lÃ­mites.

## ğŸ” Ver tambiÃ©n

- [Servicios Base](../README.md)
