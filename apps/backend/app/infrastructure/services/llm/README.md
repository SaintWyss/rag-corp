# llm
Como un **int√©rprete especializado**: toma pregunta + contexto y produce respuestas (o streaming) usando proveedores LLM concretos.

## üéØ Misi√≥n
Este subm√≥dulo implementa **adaptadores LLM** concretos para el puerto `LLMService` del dominio. Aqu√≠ viven las implementaciones reales (Google Gemini) y el fake determinista para tests/CI, con pol√≠ticas expl√≠citas como ‚Äúcontext-only‚Äù.

Sirve como hoja t√©cnica: explica qu√© proveedores existen, c√≥mo construyen prompts/versiones y c√≥mo se comportan ante errores/streaming.

### Qu√© S√ç hace
- Implementa `LLMService` con `GoogleLLMService` (Gemini) y `FakeLLMService` (determinista).
- Usa `PromptLoader` versionado para formar el prompt final (`context` + `query`).
- Aplica pol√≠tica ‚Äúcontext-only‚Äù para evitar alucinaciones si el contexto est√° vac√≠o.
- Soporta streaming (`generate_stream`) con chunks de salida controlados.

### Qu√© NO hace (y por qu√©)
- No implementa embeddings ni caching. Raz√≥n: esos adapters viven en `../` (servicios de embeddings). Consecuencia: cambios de embeddings se hacen en `app/infrastructure/services/README.md`, no aqu√≠.
- No decide qu√© provider usar en runtime. Raz√≥n: la selecci√≥n pertenece al composition root. Consecuencia: el wiring se configura en `app/container.py`.
- No expone HTTP ni DTOs. Raz√≥n: el transporte es responsabilidad de _Interfaces_. Consecuencia: los routers llaman use cases; los use cases llaman a este puerto.

## üó∫Ô∏è Mapa del territorio
| Recurso | Tipo | Responsabilidad (en humano) |
| :-- | :-- | :-- |
| `README.md` | Documento | Gu√≠a del subm√≥dulo LLM. |
| `fake_llm.py` | Archivo Python | `FakeLLMService`: respuestas y streaming deterministas para tests/CI. |
| `google_llm_service.py` | Archivo Python | `GoogleLLMService`: adapter Gemini con prompts versionados y retry. |

## ‚öôÔ∏è ¬øC√≥mo funciona por dentro?
Input ‚Üí Proceso ‚Üí Output en el flujo LLM.

- **Input:** `query` + `context` (sync) o `query` + `chunks` (streaming).
- **Proceso:** `GoogleLLMService` arma el prompt con `PromptLoader`, aplica ‚Äúcontext-only‚Äù, y llama al SDK de Gemini con retry (solo en el arranque de streaming).
- **Proceso:** `FakeLLMService` normaliza entradas, genera una respuesta hash determinista y emite chunks de tama√±o fijo.
- **Output:** `str` o `AsyncGenerator[str, None]`, y errores tipados `LLMError` si faltan inputs o credenciales.

## üîó Conexiones y roles
- **Rol arquitect√≥nico:** _Infrastructure_ (adapter a proveedor LLM).
- **Recibe √≥rdenes de:** casos de uso de `app/application/` que generan respuestas RAG.
- **Llama a:** `google.genai.Client`, `app/infrastructure/prompts/PromptLoader`, `app/infrastructure/services/retry.py`, `app/application/context_builder` (lazy import).
- **Reglas de l√≠mites:** no importar routers/DTOs; no decidir reglas de negocio.

## üë©‚Äçüíª Gu√≠a de uso (Snippets)
```python
# Por qu√©: uso recomendado v√≠a composition root (wiring centralizado).
from app.container import get_llm_service

llm = get_llm_service()
answer = llm.generate_answer(query="Q", context="CTX")
```

```python
# Por qu√©: inyectar client ayuda a testear sin tocar env vars.
from google import genai
from app.infrastructure.services.llm.google_llm_service import GoogleLLMService

client = genai.Client(api_key="dummy")
service = GoogleLLMService(client=client, model_id="gemini-1.5-flash")
```

```python
# Por qu√©: fake determinista para tests y CI sin red.
from app.infrastructure.services.llm.fake_llm import FakeLLMService

fake = FakeLLMService(stream_chunk_size=8)
```

## üß© C√≥mo extender sin romper nada
- Crear un nuevo provider en `app/infrastructure/services/llm/` que implemente `LLMService`.
- Mantener pol√≠tica ‚Äúcontext-only‚Äù y errores tipados (`LLMError`).
- Cablear el provider en `app/container.py` (selecci√≥n por settings/flags).
- Tests: unit en `apps/backend/tests/unit/` para contrato y errores; integration en `apps/backend/tests/integration/` si toca IO real.

## üÜò Troubleshooting
- **S√≠ntoma:** `LLMError: GOOGLE_API_KEY not configured`.
- **Causa probable:** no se inyect√≥ `api_key` ni existe `GOOGLE_API_KEY`.
- **D√≥nde mirar:** `google_llm_service.py` y `.env`.
- **Soluci√≥n:** definir `GOOGLE_API_KEY` o inyectar `client` en tests.
- **S√≠ntoma:** respuesta vac√≠a cuando no hay contexto.
- **Causa probable:** pol√≠tica ‚Äúcontext-only‚Äù activa.
- **D√≥nde mirar:** `GoogleLLMService._context_only_fallback()`.
- **Soluci√≥n:** asegurar contexto no vac√≠o o ajustar policy a nivel de aplicaci√≥n.
- **S√≠ntoma:** streaming corta a mitad.
- **Causa probable:** error durante el stream (no reintenta por idempotencia).
- **D√≥nde mirar:** `generate_stream` en `google_llm_service.py`.
- **Soluci√≥n:** revisar logs y reintentar a nivel de cliente.
- **S√≠ntoma:** tests flaky con streaming.
- **Causa probable:** chunk size distinto al esperado.
- **D√≥nde mirar:** `FakeLLMService(stream_chunk_size=...)`.
- **Soluci√≥n:** fijar tama√±o en tests o adaptar asserts.

## üîé Ver tambi√©n
- `../README.md`
- `../../prompts/README.md`
- `../../application/usecases/chat/README.md`
- `../../crosscutting/exceptions.py`
