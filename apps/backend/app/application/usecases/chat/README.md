# Feature: Chat & RAG

## ğŸ¯ MisiÃ³n

Contiene toda la lÃ³gica para la **Experiencia Conversacional Inteligente**.
AquÃ­ vive el algoritmo RAG (Retrieval-Augmented Generation), el manejo de historial de chat y el feedback de usuarios.

**QuÃ© SÃ hace:**

- Implementa el flujo RAG: Pregunta -> Embedding -> BÃºsqueda -> Prompt -> LLM.
- Maneja streaming de respuestas (Token a Token).
- Gestiona sesiones de chat (Crear, Listar, Borrar).
- Registra votos/feedback de usuarios.

**AnalogÃ­a:**
Es el Bibliotecario experto que no solo busca el libro, sino que lo lee y te resume la respuesta a tu pregunta.

## ğŸ—ºï¸ Mapa del territorio

| Recurso                        | Tipo       | Responsabilidad (en humano)                                             |
| :----------------------------- | :--------- | :---------------------------------------------------------------------- |
| `answer_query.py`              | ğŸ Archivo | **RAG EstÃ¡ndar**. Respuesta completa de una sola vez (stateless).       |
| `answer_query_with_history.py` | ğŸ Archivo | **Chat RAG**. Respuesta considerando mensajes anteriores.               |
| `chat_utils.py`                | ğŸ Archivo | Helpers para formatear mensajes del historial para el LLM.              |
| `clear_conversation.py`        | ğŸ Archivo | Borra mensajes de una sesiÃ³n.                                           |
| `create_conversation.py`       | ğŸ Archivo | Inicia una nueva sesiÃ³n de chat vacÃ­a.                                  |
| `get_conversation_history.py`  | ğŸ Archivo | Recupera los mensajes previos de una sesiÃ³n.                            |
| `record_answer_audit.py`       | ğŸ Archivo | Guarda trazas de auditorÃ­a de respuestas generadas.                     |
| `search_chunks.py`             | ğŸ Archivo | **Retrieval Only**. Solo busca fragmentos relevantes sin llamar al LLM. |
| `stream_answer_query.py`       | ğŸ Archivo | **Streaming RAG**. Generador que emite tokens en tiempo real.           |
| `vote_answer.py`               | ğŸ Archivo | Registra si una respuesta fue Ãºtil (ğŸ‘/ğŸ‘).                             |

## âš™ï¸ Â¿CÃ³mo funciona por dentro?

### Flujo RAG (`answer_query.py`)

1.  **Rewrite:** Reescribe la pregunta si es ambigua.
2.  **Embed:** Convierte la pregunta a vector.
3.  **Retrieve:** Busca chunks similares en `DocumentRepository`.
4.  **Rerank:** Reordena los chunks por relevancia.
5.  **Generate:** Construye prompt con `ContextBuilder` e invoca al `LLMService`.

### Streaming

Usa generadores de Python (`yield`) para pasar los tokens desde el LLM hasta la API a medida que se generan.

## ğŸ”— Conexiones y roles

- **Rol ArquitectÃ³nico:** Use Cases (Chat Feature).
- **Colabora con:** `LLMService`, `EmbeddingService`, `DocumentRepository`.

## ğŸ‘©â€ğŸ’» GuÃ­a de uso (Snippets)

### Ejecutar una bÃºsqueda simple (SearchChunks)

```python
use_case = SearchChunksUseCase(document_repo, embedding_service)
results = use_case.execute(
    SearchChunksInput(query="polÃ­tica de gastos", workspace_id=ws_id)
)
for chunk in results.chunks:
    print(chunk.content)
```

## ğŸ§© CÃ³mo extender sin romper nada

1.  **Nuevos Modelos:** Si quieres soportar "Thinking Models", modifica `answer_query.py` o crea un `think_answer_query.py`.
2.  **Historial:** La gestiÃ³n de memoria estÃ¡ en `chat_utils.py`.

## ğŸ†˜ Troubleshooting

- **SÃ­ntoma:** Respuestas lentas.
  - **Causa:** El modelo LLM es muy grande o el Reranker estÃ¡ tardando.
- **SÃ­ntoma:** "I don't know".
  - **Causa:** El retrieval no trajo chunks relevantes (revisar embeddings).

## ğŸ” Ver tambiÃ©n

- [Use Case Hub](../README.md)
