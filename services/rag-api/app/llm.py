"""
Name: Google Gemini LLM Service

Responsibilities:
  - Generate RAG answers using Gemini 1.5 Flash model
  - Construct prompts with retrieved context + user query
  - Apply business rules (answer only based on context)
  - Handle generation errors with fallback to clear error messages

Collaborators:
  - google.generativeai: Gemini SDK
  - GOOGLE_API_KEY: Credentials shared with embeddings service

Constraints:
  - Hardcoded prompt engineering (not externally configurable)
  - No control over temperature, top_p, max_tokens (uses defaults)
  - Responses limited to Spanish (by prompt)
  - No streaming responses (generates complete answer before returning)

Notes:
  - Model gemini-1.5-flash chosen for cost/quality balance
  - Prompt designed to prevent hallucinations (strict context restriction)
  - Critical rule: If insufficient context, must say "No information available"

Security:
  - Don't log complete prompts (may contain sensitive context data)
  - Validate context doesn't exceed model limits (128K tokens)

Performance:
  - Typical latency: 1-3 seconds for 50-200 word responses
  - Consider response caching for identical queries (future)
"""
import os
import google.generativeai as genai
from .logger import logger
from .exceptions import LLMError

# R: Load Google API key from environment (shared with embeddings)
API_KEY = os.getenv("GOOGLE_API_KEY")
if API_KEY:
    # R: Configure global API client
    genai.configure(api_key=API_KEY)

def generate_rag_answer(query: str, context_chunks: list[str]) -> str:
    """
    R: Generate answer based on query + retrieved context.
    
    Args:
        query: User's question
        context_chunks: List of relevant fragments retrieved from DB
    
    Returns:
        Answer generated by Gemini (Spanish, based on context)
    
    Notes:
        - If context_chunks is empty, returns "not found" message
        - Prompt includes instructions to avoid answering outside context
        - Response format: professional, concise, in Spanish
    
    Raises:
        LLMError: If API key not configured or generation fails
    """
    if not API_KEY:
        logger.error("GOOGLE_API_KEY not configured for LLM")
        raise LLMError("API Key not configured")

    # R: Concatenate retrieved chunks into single context string
    context_text = "\n\n".join(context_chunks)
    
    # R: Construct prompt with context and business rules
    prompt = f"""
    Act as an expert assistant for RAG Corp company.
    Your mission is to answer the user's question based EXCLUSIVELY on the context provided below.
    
    Rules:
    1. If the answer is not in the context, say "I don't have enough information in my documents".
    2. Be concise and professional.
    3. Always respond in Spanish.

    --- CONTEXT ---
    {context_text}
    ----------------
    
    Question: {query}
    Answer:
    """
    
    try:
        # R: Initialize Gemini 1.5 Flash model (cost-effective choice)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # R: Generate response using constructed prompt
        response = model.generate_content(prompt)
        logger.info("LLM response generated successfully")
        
        # R: Return cleaned response text
        return response.text.strip()
    except LLMError:
        raise
    except Exception as e:
        logger.error(f"LLM generation failed: {e}")
        raise LLMError(f"Failed to generate response: {e}")
